{
  # refer to https://github.com/wey-gu/fraud-detection-datagen
  # for the data generation and importation to NebulaGraph

  # Spark relation config
  spark: {
    app: {
        name: louvain
        # spark.app.partitionNum
        partitionNum:10
    }
    master:local
  }

  data: {
    # data source. optional of nebula,csv,json
    source: nebula
    # data sink, means the algorithm result will be write into this sink. optional of nebula,csv,text
    sink: csv
    # if your algorithm needs weight
    hasWeight: false
  }

  # Nebula Graph relation config
  nebula: {
    # algo's data source from Nebula. If data.source is nebula, then this nebula.read config can be valid.
    read: {
        # Nebula metad server address, multiple addresses are split by English comma
        metaAddress: "metad0:9559"
        # Nebula space
        space: frauddetection
        # Nebula edge types, multiple labels means that data from multiple edges will union together
        labels: ["with_phone_num", "is_related_to", "worked_for", "used_device"]
        # Nebula edge property name for each edge type, this property will be as weight col for algorithm.
        # Make sure the weightCols are corresponding to labels.
        weightCols: []
    }

    # algo result sink into Nebula. If data.sink is nebula, then this nebula.write config can be valid.
    write:{
        # Nebula graphd server addressï¼Œ multiple addresses are split by English comma
        graphAddress: "graphd:9669"
        # Nebula metad server address, multiple addresses are split by English comma
        metaAddress: "metad0:9559"
        type:insert
        user:root
        pswd:nebula
        # Nebula space name
        space:frauddetection
        # Nebula tag name, the algorithm result will be write into this tag
        tag:louvain
    }
  }

  local: {
    read:{
        filePath: "hdfs://hadoop:9000/dummy.txt"
        # srcId column
        srcId:"_c0"
        # dstId column
        dstId:"_c1"
        # weight column
        #weight: "col3"
        # if csv file has header
        header: false
        # csv file's delimiter
        delimiter:"\t"
    }

    # algo result sink into local file. If data.sink is csv or text, then this local.write can be valid.
    write:{
        resultPath:/output/
    }
  }


  algorithm: {
    executeAlgo: louvain

    # louvain parameter
    louvain: {
        maxIter: 20
        internalIter: 10
        tol: 0.5
    }
 }
}