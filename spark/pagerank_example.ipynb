{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6547353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe327b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PageRankExample\").getOrCreate()\n",
    "jspark = spark._jsparkSession\n",
    "\n",
    "from py4j.java_gateway import java_import\n",
    "# import \"com.vesoft.nebula.algorithm.config.SparkConfig\"\n",
    "java_import(spark._jvm, \"com.vesoft.nebula.algorithm.config.SparkConfig\")\n",
    "\n",
    "# import \"com.vesoft.nebula.algorithm.config.PRConfig\"\n",
    "java_import(spark._jvm, \"com.vesoft.nebula.algorithm.config.PRConfig\")\n",
    "\n",
    "# import \"com.vesoft.nebula.algorithm.lib.PageRankAlgo\"\n",
    "java_import(spark._jvm, \"com.vesoft.nebula.algorithm.lib.PageRankAlgo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0feb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 0: read graph data for one type of edge, scan from all storaged instances\n",
    "df = spark.read.format(\n",
    "  \"com.vesoft.nebula.connector.NebulaDataSource\").option(\n",
    "    \"type\", \"edge\").option(\n",
    "    \"spaceName\", \"basketballplayer\").option(\n",
    "    \"label\", \"follow\").option(\n",
    "    \"returnCols\", \"degree\").option(\n",
    "    \"metaAddress\", \"metad0:9559\").option(\n",
    "    \"partitionNumber\", 1).load()\n",
    "\n",
    "# option 1: read graph data with ngql, get data from graphd\n",
    "df = spark.read.format(\n",
    "  \"com.vesoft.nebula.connector.NebulaDataSource\").option(\n",
    "    \"type\", \"edge\").option(\n",
    "    \"spaceName\", \"basketballplayer\").option(\n",
    "    \"label\", \"follow\").option(\n",
    "    \"returnCols\", \"degree\").option(\n",
    "    \"metaAddress\", \"metad0:9559\").option(\n",
    "    \"graphAddress\", \"graphd:9669\").option(\n",
    "    \"ngql\", \"MATCH ()-[e:follow]->() return e LIMIT 1000\").option(\n",
    "    \"partitionNumber\", 1).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31871548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prConfig = PRConfig(3, 0.85)\n",
    "prConfig = spark._jvm.PRConfig(3, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prResult = PageRankAlgo.apply(spark, df, prConfig, False)\n",
    "prResult = spark._jvm.PageRankAlgo.apply(jspark, df._jdf, prConfig, False)\n",
    "\n",
    "# We will fail in this step as our graph vertex ID is not numeric\n",
    "# let's convert vertex ID into number first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4c3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, col\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0bace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_id_to_long_id(df):\n",
    "    src_id_df = df.select(\"_srcId\").withColumnRenamed(\"_srcId\", \"id\")\n",
    "    dst_id_df = df.select(\"_dstId\").withColumnRenamed(\"_dstId\", \"id\")\n",
    "    id_df = src_id_df.union(dst_id_df).distinct()\n",
    "    encode_id = id_df.withColumn(\"encodedId\", dense_rank().over(Window.orderBy(\"id\")))\n",
    "    encode_id.write.option(\"header\", True).csv(\"file:///tmp/encodeId.csv\")\n",
    "    src_join_df = df.join(encode_id, df._srcId == encode_id.id)\\\n",
    "        .drop(\"_srcId\")\\\n",
    "        .drop(\"id\")\\\n",
    "        .withColumnRenamed(\"encodedId\", \"_srcId\")\n",
    "    dst_join_df = src_join_df.join(encode_id, src_join_df._dstId == encode_id.id)\\\n",
    "        .drop(\"_dstId\")\\\n",
    "        .drop(\"id\")\\\n",
    "        .drop(\"_rank\")\\\n",
    "        .drop(\"degree\")\\\n",
    "        .withColumnRenamed(\"encodedId\", \"_dstId\")\n",
    "    \n",
    "    return dst_join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ddf1bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/13 08:48:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/13 08:48:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/13 08:48:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/13 08:48:25 WARN BlockManager: Block rdd_38_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|_id|          pagerank|\n",
      "+---+------------------+\n",
      "| 20|1.0467769505659303|\n",
      "| 19| 1.107604367969783|\n",
      "| 13|0.9985417030429499|\n",
      "| 41| 0.907496696364626|\n",
      "| 39|0.9310679357433543|\n",
      "|  2|1.1995083922141179|\n",
      "| 21|1.1241377422857828|\n",
      "|  4| 0.922979999502629|\n",
      "| 15|1.0374131525148615|\n",
      "| 16| 1.173844971046562|\n",
      "| 34|0.9416714112085575|\n",
      "| 25| 1.020692343026497|\n",
      "| 22|0.8661097076682367|\n",
      "| 28|0.8661097076682367|\n",
      "| 29|0.8661097076682367|\n",
      "| 11|0.8661097076682367|\n",
      "| 14|1.2091869945122087|\n",
      "| 32| 1.009041691134994|\n",
      "| 35| 0.922979999502629|\n",
      "| 36| 1.203961967699571|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_int = convert_string_id_to_long_id(df)\n",
    "prResult = spark._jvm.PageRankAlgo.apply(jspark, df_int._jdf, prConfig, False)\n",
    "\n",
    "# pageren result, but this is in vid type\n",
    "prResult.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2a56eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|       id|encodedId|\n",
      "+---------+---------+\n",
      "|player100|        1|\n",
      "|player101|        2|\n",
      "|player102|        3|\n",
      "|player103|        4|\n",
      "|player104|        5|\n",
      "|player105|        6|\n",
      "|player106|        7|\n",
      "|player107|        8|\n",
      "|player108|        9|\n",
      "|player109|       10|\n",
      "|player113|       11|\n",
      "|player114|       12|\n",
      "|player115|       13|\n",
      "|player116|       14|\n",
      "|player117|       15|\n",
      "|player118|       16|\n",
      "|player119|       17|\n",
      "|player120|       18|\n",
      "|player121|       19|\n",
      "|player124|       20|\n",
      "+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is the mapping of the vid and the encodedId\n",
    "mapping = spark.read.option(\"header\", True).csv(\"file:///tmp/encodeId.csv\")\n",
    "\n",
    "mapping.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
